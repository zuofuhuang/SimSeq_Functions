---
title: "New Model"
author: "Zuofu Huang"
output: html_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(TraMineR)
library(performance)
library(GLMMadaptive)
library(pscl)
library(rbenchmark)
library(foreach)
library(doParallel)
library(fastcluster)
library(fitdistrplus)
library(truncnorm)
library(MASS)
library(clValid)
```

```{r}
# source("Functions.R")
source("Functions_start12am.R")
# source("Functions_start_with_clusters.R")
```

```{r}
# data <- read.csv("wide_biweekly_v2.csv")[,-1]
# data <- data %>%  # Only use sequences with less than 1/4 of the day missing?
#   filter(total_min > 1440*(3/4))
# data$record_id <- as.factor(data$record_id)
# 
# wide_data2 <- read.csv("wide_data2.csv")[,-1]
# wide_data2 <- wide_data2 %>%
#   mutate(start_min = floor(as.numeric(hms(start_timeonly)-1)/60) + 1) %>% # 1 = 00:00, 1440 = 23:59
#   mutate(end_min = floor(as.numeric(hms(end_timeonly)-1)/60) + 1) %>%
#   mutate(dur = end_min - start_min)
# 
# # We have 2900 + dates. Here we only extract day sequences with a cor1ponding biweekly survey. If we want all the days, we can replace data with wide_data_2 in creating `new` dataset.
# new <- data %>% 
#   dplyr::select(record_id, date)
# new <- cbind(new, data.frame(matrix(NA, nrow = nrow(new), ncol = 1440)))
# colnames(new) <- c("record_id", "date", format(seq(as.POSIXct("00:00:00", format = "%T"), 
#            as.POSIXct("23:59:00", format = "%T"), by = "1 min"), "%H:%M"))
# 
# for (i in 1:nrow(new)){
#   id <- new$record_id[i]
#   date <- new$date[i]
#   subset <- wide_data2 %>%
#     filter(record_id == id) %>%
#     filter(start_dateonly == date)
#   
#   for(j in 1:nrow(subset)){
#     start <- subset$start_min[j] + 3
#     end <- subset$end_min[j] + 2
#     if(end - start >= 0){
#       subtype <- subset$subtype[j]
#       new[i, start:end] <- subtype
#     }
#   }
# }
# 
# # Of course, the lengths of sequences are different. How would we adjust for that?
# # It's fine (actually more realistic if we don't adjust for that)
# sequences <- new[,-c(1,2)]
```

```{r}
sequences <- read.csv("sequences.csv")[,-1]
seq_distances <- read.csv("1929sequences_dist_om.csv")[,-1]

colnames(sequences) <- c(format(seq(as.POSIXct("00:00:00", format = "%T"), 
                 as.POSIXct("23:59:00", format = "%T"), by = "1 min"), "%H:%M"))
```


## Simulate sequences

What parameters of sequences change the nature of sequences? (State, ordering, duration)

**Haven't solved the problem of missing data**


```{r}
# Start with a seed anyway
set.seed(2022)

# About 4-5 seconds per sequence
# simulated_sequences <- simulate_multiple_sequences(sequences, 500)

# For the sake of generating sequences only: Change NA to N/A
# sequences[is.na(sequences)] <- "N/A"

# About 1/3 time as before
# simulated_sequences_noCluster <- parallel_simulate_multiple_sequences(sequences, 1929)
# simulated_sequences_bySeparate <- parallel_simulate_multiple_sequences(sequences, fit_real, 1929)
# simulated_sequences_byDunn <- parallel_simulate_multiple_sequences(sequences, cluster_assignment, 1929)

sim_seq_noCluster <- read.csv("simulated_sequences_no_cluster.csv")[,-1]
sim_seq_Dunn <- read.csv("simulated_sequences_by_Dunn.csv")[,-1]
sim_seq_Separate <- read.csv("simulated_sequences_bySeparate.csv")[,-1]

dist_mat_noCluster <- read.csv("simulated_seq_noCluster_dist_om.csv")[,-1]
dist_mat_byDunn <- read.csv("simulated_seq_byDunn_dist_om.csv")[,-1]
dist_mat_bySeparate <- read.csv("simulated_seq_bySeparate_dist_om.csv")[,-1]
```

    
## Pairwise distances between original sample and simulated sequences

Pairwise distances of the original data (1929) using OM are stored.

```{r}
original_sample <- sequences[sample(1:nrow(sequences), 100, replace = T),] # This is a stslist
colnames(simulated_sequences) <- colnames(original_sample)
combined <- rbind(original_sample, simulated_sequences)
combined_seq <- seqdef(combined)
```


```{r}
# Pairwise distances between sample of the original sequences and simulated sequences
# Code is the same as code in the next chunk for the original data
costs <- seqcost(combined_seq, method = "TRATE", with.missing = TRUE)
distances_euclid <- seqdist(combined_seq, method = "EUCLID", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
distances_om <- seqdist(combined_seq, method = "OM", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
```





## Hierarchical Clustering and distribution plots by cluster

```{r}
# We can use the matrices along the diagonal of the distance matrix.
hclust_real <- stats::hclust(as.dist(distances_om[1:100,1:100]), method = "average")
fit_real <- cutree(hclust_real, k = 4)

hclust_sim <- stats::hclust(as.dist(distances_om[101:200,101:200]), method = "average")
fit_sim <- cutree(hclust_sim, k = 6)

hclust_full <- fastcluster::hclust(as.dist(distances3), method = "average")
fit_full <- cutree(hclust_full, k = 20)
```

```{r}
# The original_sample is less important, because we are mostly trying to see whether the simulated has the same distribution as the original data.
temp <- original_sample[which(fit_real == 1),] # 40
g1_real <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- original_sample[which(fit_real == 2),] # 53 all home
g2_real <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- original_sample[which(fit_real == 3),] # 4
g3_real <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- original_sample[which(fit_real == 4),] # 3
g4_real <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)

temp <- simulated_sequences[which(fit_sim == 1),] # 65 all home
g1_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- simulated_sequences[which(fit_sim == 2),] # 29
g2_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- simulated_sequences[which(fit_sim == 3),] # 2
g3_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- simulated_sequences[which(fit_sim == 4),] # 1
g4_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- simulated_sequences[which(fit_sim == 5),] # 2
g5_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- simulated_sequences[which(fit_sim == 6),] # 1
g6_sim <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)

temp <- sequences[which(fit_full == 1),] # 877
g1_all <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
temp <- sequences[which(fit_full == 2),] # 911 all home
g2_all <- seqplot(seqdef(temp), border = NA, type = "d", cex.legend = 0.5)
```




## Make sure that the method adequately generates sequences with certain characteristics

```{r}
# Say, we want to generate sequences with at least 6 hours of work.
# Be careful here. We cannot use base R, because it will preserve the original row number,
# which creates huge problems when we are doing unlist(apply)
workdays <- sequences %>%
  filter(rowSums(. == "WORK", na.rm = TRUE) > 240) # average work time 565, sd 117

# average work time 565, sd 240. The mean is the same, but it's unstable.
# 82 out of 100 have at least 6 hours of work.
filter_first <- parallel_simulate_multiple_sequences(workdays, 100)
rowsum_first <- rowSums(filter_first == "WORK", na.rm = TRUE)

# To get 100 qualifying work days, we simulate 500. In this run, the first 340 contain 100 valid workdays.
# mean 605, sd 186
filter_second <- parallel_simulate_multiple_sequences(sequences, 500)
filter_second <- filter_second[1:340,]
filter_second <- filter_second[rowSums(filter_second[1:350,] == "WORK", na.rm = TRUE) > 360,]
rowsum_second <- rowSums(filter_second == "WORK", na.rm = TRUE) 
```

Overall trend is fine, but the tails are larger. Structurally, the model is not fully complex, so there is not sufficient tool to capture the structure.


```{r}
reorder_cluster_assignment <- function(assignment){
  tbl <- table(assignment)
  groups <- length(tbl)
  
  list <- list()
  for (i in 1:groups){
    list[[i]] <- which(assignment == i)
  }
  
  sorted_tbl <- sort(tbl, decreasing = TRUE)
  cluster_num_by_size <- as.numeric(names(sorted_tbl))
  
  for (j in 1:groups){
    assignment[list[[(cluster_num_by_size[j])]]] <- j
  }
  
  return(assignment)
}

cluster_assignment <- create_clusters_Dunn(dist_mat_byDunn)
cluster_assignment <- reorder_cluster_assignment(cluster_assignment)
cluster_assignment[cluster_assignment > 2] <- 3

hclust_real <- stats::hclust(as.dist(dist_mat_bySeparate), method = "average")
fit_real <- cutree(hclust_real, k = 15)
fit_real <- reorder_cluster_assignment(fit_real)
fit_real[fit_real > 2] <- 3
```



```{r}
# Distribution of number of transitions each day
transitions_per_day <- function(sequence){
  return(length((rle(sequence)$length)))
}

# sim_seq_Dunn[is.na(sim_seq_Dunn)] <- "N/A"
# la <- apply(sim_seq_Dunn, 1, transitions_per_day)
```


## Some code

### Code for distribution plots

```{r}
sim_seq_Dunn[sim_seq_Dunn == "N/A"] <- NA
seqplot(seqdef(sim_seq_Dunn), border = NA, type = "d", cex.legend = 0.5)
```

### Code for calculating distance matrix

```{r}
# simulated_sequences_noCluster[simulated_sequences_noCluster == "N/A"] <- NA
# seq_costs_noCluster <- seqcost(seqdef(simulated_sequences_noCluster), method = "TRATE", with.missing = TRUE)
# seq_distances_noCluster <- seqdist(seqdef(simulated_sequences_noCluster), method = "OM", 
#                          indel = seq_costs_noCluster$indel, sm = seq_costs_noCluster$sm, with.missing = TRUE)
# 
# simulated_sequences_bySeparate[simulated_sequences_bySeparate == "N/A"] <- NA
# seq_costs_bySeparate <- seqcost(seqdef(simulated_sequences_bySeparate), method = "TRATE", with.missing = TRUE)
# seq_distances_bySeparate <- seqdist(seqdef(simulated_sequences_bySeparate), method = "OM", 
#                          indel = seq_costs_bySeparate$indel, sm = seq_costs_bySeparate$sm, with.missing = TRUE)
# 
# simulated_sequences_byDunn[simulated_sequences_byDunn == "N/A"] <- NA
# seq_costs_byDunn <- seqcost(seqdef(simulated_sequences_byDunn), method = "TRATE", with.missing = TRUE)
# seq_distances_byDunn <- seqdist(seqdef(simulated_sequences_byDunn), method = "OM",
#                          indel = seq_costs_byDunn$indel, sm = seq_costs_byDunn$sm, with.missing = TRUE)
```



## Change the length of sequence to 720

```{r}
set.seed(2023)
short <- sequences[,1:720]

simulated_short <- parallel_simulate_multiple_sequences(short, 100)
```

```{r}
original_short <- short[sample(1:nrow(short), 100, replace = T),] # This is a stslist
colnames(simulated_short) <- colnames(original_short)
combined_short <- rbind(original_short, simulated_short)
```

```{r}
combined_seq_short <- seqdef(combined_short)
costs <- seqcost(combined_seq_short, method = "TRATE", with.missing = TRUE)
distances <- seqdist(combined_seq_short, method = "EUCLID", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
distances2 <- seqdist(combined_seq_short, method = "CHI2", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
distances3 <- seqdist(combined_seq_short, method = "OM", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
distances4 <- seqdist(combined_seq_short, method = "OMstran", indel = costs$indel, sm = costs$sm, otto = 1, with.missing = TRUE)
distances5 <- seqdist(combined_seq_short, method = "OMspell", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
distances6 <- seqdist(combined_seq_short, method = "OMslen", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
```


## Change the length of sequence 360

```{r}
set.seed(2023)
short <- sequences[,361:720]

simulated_short <- parallel_simulate_multiple_sequences(short, 100)
```

```{r}
original_short <- short[sample(1:nrow(short), 100, replace = T),] # This is a stslist
colnames(simulated_short) <- colnames(original_short)
combined_short <- rbind(original_short, simulated_short)
```

```{r}
# combined_seq_short <- seqdef(combined_short)
# costs <- seqcost(combined_seq_short, method = "TRATE", with.missing = TRUE)
# distances <- seqdist(combined_seq_short, method = "EUCLID", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
# distances2 <- seqdist(combined_seq_short, method = "CHI2", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
# distances3 <- seqdist(combined_seq_short, method = "OM", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
# distances4 <- seqdist(combined_seq_short, method = "OMstran", indel = costs$indel, sm = costs$sm, otto = 1, with.missing = TRUE)
# distances5 <- seqdist(combined_seq_short, method = "OMspell", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
# distances6 <- seqdist(combined_seq_short, method = "OMslen", indel = costs$indel, sm = costs$sm, with.missing = TRUE)
```

```{r}
seqdplot(seqdef(short), border = NA, with.legend = "auto")

seqdplot(seqdef(simulated_short), border = NA, with.legend = "auto")

seqdplot(seqdef(original_short), border = NA, with.legend = "auto", space = 0)
```

```{r}
mean(distances6[1:100,1:100])
sd(distances6[1:100,1:100])

mean(distances6[1:100,101:200])
sd(distances6[1:100,101:200])

mean(distances6[101:200,101:200])
sd(distances6[101:200,101:200])
```





The steps above are very heuristic. Building a local transition matrix is a means to help with the smoothing problem. 

Alternatively, we could have the activities overlap from each other. Then, we can use tools including the transition matrix to smooth over. (But then there are a lot of edge cases around the overlapping pattern that we have to take care of.)

Lastly, we also want to take into account that the day doesn't necessarily start with 00:00 if someone sleeps at 1am.

The other idea we talked about during the meeting is to view the sequence as a collection of time series. When simulating time series, we can use the residuals to quantify. When simulating sequences, we can quantify our changed with a collection of residuals.

Alternatively, can we use some strategies with multivariate time series in economics? https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/


